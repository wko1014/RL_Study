{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Planning and Learning with Tabular Methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wko1014/RL_Study/blob/main/notes/Planning_and_Learning_with_Tabular_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBuQ6IVB6bJd"
      },
      "source": [
        "# Import APIs\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# APIs for annimation\n",
        "from IPython.display import clear_output\n",
        "from time import sleep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcvHFfhJ6g1m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ad62fe54-8cd3-4625-b729-332dfcaa61a3"
      },
      "source": [
        "# Call Taxi environment\n",
        "env = gym.make('Taxi-v2').env\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7ScLeDo6iLs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3edc6132-4309-43c4-a3b4-1915bf8c745d"
      },
      "source": [
        "%%time\n",
        "# Now we implement tabular Dyna-Q algorithm.\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "num_iterations = 1000\n",
        "alpha, epsilon, gamma, lambda_ = .4, .1, .9, .9\n",
        "n = 5\n",
        "# Initialize\n",
        "Q = np.zeros([num_states, num_actions])\n",
        "model = np.zeros([num_states, num_actions, 2])\n",
        "\n",
        "# Repeat\n",
        "for iter in range(num_iterations):\n",
        "  S = env.reset() # Initialize S\n",
        "  trajectory = np.zeros([num_states, num_actions])\n",
        "  # Loop for each step of episode\n",
        "  done = False\n",
        "  while not done:\n",
        "    env.s = S\n",
        "    # Epsilon-greedy XX\n",
        "    if np.random.rand() < epsilon:\n",
        "      A = env.action_space.sample() # take a random action\n",
        "    else:\n",
        "      A = np.argmax(Q[S, :])\n",
        "    S_prime, R, done, info = env.step(int(A))\n",
        "    Q[S, A] += alpha * (R + gamma * np.max(Q[S_prime, :]) - Q[S, A]) # Direct RL\n",
        "    model[S, A, 0], model[S, A, 1] = R, int(S_prime) # Model Learning (Assuming deterministic env.)\n",
        "    trajectory[S, A] = 1\n",
        "    current = S_prime\n",
        "    for i in range(n): # if n=0, this algorithm do not have planning.\n",
        "      S = np.random.choice(np.nonzero(np.sum(trajectory, -1))[0]) # random previously observed state\n",
        "      A = int(np.random.choice(trajectory[S, :])) # random action previously taken in S\n",
        "      R, S_prime = model[S, A, 0], int(model[S, A, 1])\n",
        "      Q[S, A] += alpha * (R + gamma * np.max(Q[S_prime, :])-Q[S, A])\n",
        "    S = current"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.88 s, sys: 2.88 ms, total: 5.89 s\n",
            "Wall time: 5.89 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrhjYYRc6kUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d4250635-ace1-446f-fbd6-6f7af74ef9a3"
      },
      "source": [
        "# Now, the agent is learnt with tabular Dyna-Q algorithm.\n",
        "\n",
        "# Initialize\n",
        "state = env.reset()\n",
        "\n",
        "epochs, penalties, reward = 0, 0, 0\n",
        "# For animation\n",
        "animation_dyna_Q = []\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  action = np.argmax(Q[state, :])\n",
        "  state, reward, done, info = env.step(action)\n",
        "  \n",
        "  animation_dyna_Q.append({\"frame\": env.render(mode=\"ansi\"), \"state\":state,\n",
        "                    \"action\":action, \"reward\":reward})\n",
        "  epochs += 1\n",
        "\n",
        "print(\"The agent used {} timesteps for delivery.\".format(epochs))\n",
        "print(\"The agent got {} penalties.\".format(penalties))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The agent used 14 timesteps for delivery.\n",
            "The agent got 0 penalties.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u37GGP8s6mYw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "909a5f34-a29b-458b-c5d5-b2c231ab03cf"
      },
      "source": [
        "%%time\n",
        "# To animate, we define a function.\n",
        "def animating(frames, time_per_frame):\n",
        "  for i, frame in enumerate(frames):\n",
        "    clear_output(wait=True)\n",
        "    print(frame[\"frame\"].getvalue())\n",
        "    print(f\"Timesteps: {i}\")\n",
        "    print(f\"State: {frame['state']}\")\n",
        "    print(f\"Action: {frame['action']}\")\n",
        "    print(f\"Reward: {frame['reward']}\")\n",
        "    sleep(time_per_frame)\n",
        "    \n",
        "animating(animation_dyna_Q, 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timesteps: 13\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n",
            "CPU times: user 38.5 ms, sys: 7.32 ms, total: 45.8 ms\n",
            "Wall time: 7.03 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN2cvyz86n9F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}