{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Temporal-Difference_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wko1014/RL_Study/blob/main/notes/tmpeaifeaTemporal_Difference_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-WgI7RqGKB_"
      },
      "source": [
        "# Import APIs\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# APIs for annimation\n",
        "from IPython.display import clear_output\n",
        "from time import sleep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1JHEJ_EGS3M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1f6fefe1-039f-4ec5-a930-97d8e0374382"
      },
      "source": [
        "# Call Taxi environment\n",
        "env = gym.make('Taxi-v2').env\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwH2BLTUGejC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "513193cc-ac08-4c11-d5aa-c76cfc6142b6"
      },
      "source": [
        "%%time\n",
        "# Let us implement on-policy TD control, Sarsa algorithm.\n",
        "# Define hyperparameters\n",
        "alpha = .4\n",
        "epsilon = .1\n",
        "gamma = .9\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "num_iterations = 10000\n",
        "\n",
        "# Initialize\n",
        "Q = np.zeros([num_states, num_actions])\n",
        "\n",
        "# Loop for each episode\n",
        "for iter in range(num_iterations):\n",
        "  S = env.reset() # Initialize S\n",
        "  \n",
        "  # Epsilon-greedy XX\n",
        "  if np.random.rand() < epsilon:\n",
        "    A = env.action_space.sample() # take a random action\n",
        "  else:\n",
        "    A = np.argmax(Q[S, :])\n",
        "  \n",
        "  # Loop for each step of episode\n",
        "  done = False\n",
        "  while not done:\n",
        "    S_prime, R, done, info = env.step(int(A))\n",
        "    \n",
        "    # Epsilon-greedy XX\n",
        "    if np.random.rand() < epsilon:\n",
        "      A_prime = env.action_space.sample()\n",
        "    else:\n",
        "      A_prime = np.argmax(Q[S_prime, :])\n",
        "    Q[S, A] += alpha * (R + gamma * Q[S_prime, A_prime] - Q[S, A])\n",
        "    S, A = S_prime, A_prime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.02 s, sys: 2.2 ms, total: 3.03 s\n",
            "Wall time: 3.03 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFLaABpMRhRa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "752571c7-d9ec-4e47-e25f-4c6198d29f15"
      },
      "source": [
        "# Now, the agent is learnt with Sarsa algorithm.\n",
        "\n",
        "# Initialize\n",
        "state = env.reset()\n",
        "\n",
        "epochs, penalties, reward = 0, 0, 0\n",
        "# For animation\n",
        "animation_sarsa = []\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  action = np.argmax(Q[state, :])\n",
        "  state, reward, done, info = env.step(action)\n",
        "  \n",
        "  animation_sarsa.append({\"frame\": env.render(mode=\"ansi\"), \"state\":state,\n",
        "                    \"action\":action, \"reward\":reward})\n",
        "  epochs += 1\n",
        "\n",
        "print(\"The agent used {} timesteps for delivery.\".format(epochs))\n",
        "print(\"The agent got {} penalties.\".format(penalties))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The agent used 10 timesteps for delivery.\n",
            "The agent got 0 penalties.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GooELI_lSF77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "bb714ef4-b2a5-4f27-8a2c-0d8bfc8e90f3"
      },
      "source": [
        "%%time\n",
        "# To animate, we define a function.\n",
        "def animating(frames, time_per_frame):\n",
        "  for i, frame in enumerate(frames):\n",
        "    clear_output(wait=True)\n",
        "    print(frame[\"frame\"].getvalue())\n",
        "    print(f\"Timesteps: {i}\")\n",
        "    print(f\"State: {frame['state']}\")\n",
        "    print(f\"Action: {frame['action']}\")\n",
        "    print(f\"Reward: {frame['reward']}\")\n",
        "    sleep(time_per_frame)\n",
        "    \n",
        "animating(animation_sarsa, 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timesteps: 9\n",
            "State: 410\n",
            "Action: 5\n",
            "Reward: 20\n",
            "CPU times: user 32.1 ms, sys: 10.1 ms, total: 42.2 ms\n",
            "Wall time: 5.03 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l3IxS31cvLA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ab899e1e-69d6-4c85-d87b-428cb427c3fc"
      },
      "source": [
        "%%time\n",
        "# Let us implement off-policy TD control, Q-learning algorithm.\n",
        "# Define hyperparameters\n",
        "alpha = .4\n",
        "epsilon = .1\n",
        "gamma = .9\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "num_iterations = 1000\n",
        "\n",
        "# Initialize\n",
        "Q = np.zeros([num_states, num_actions])\n",
        "\n",
        "# Loop for each episode\n",
        "for iter in range(num_iterations):\n",
        "  S = env.reset() # Initialize S\n",
        "  \n",
        "  # Loop for each step of episode\n",
        "  done = False\n",
        "  while not done:\n",
        "    # Epsilon-greedy XX\n",
        "    if np.random.rand() < epsilon:\n",
        "      A = env.action_space.sample() # take a random action\n",
        "    else:\n",
        "      A = np.argmax(Q[S, :])\n",
        "    S_prime, R, done, info = env.step(int(A))\n",
        "    Q[S, A] += alpha * (R + gamma * np.max(Q[S_prime, :]) - Q[S, A])\n",
        "    S = S_prime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 782 ms, sys: 0 ns, total: 782 ms\n",
            "Wall time: 782 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHI8jCuKfKSW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "97df0f01-b028-419a-babe-26e37aac44ef"
      },
      "source": [
        "# Now, the agent is learnt with Q-learning algorithm.\n",
        "\n",
        "# Initialize\n",
        "state = env.reset()\n",
        "\n",
        "epochs, penalties, reward = 0, 0, 0\n",
        "# For animation\n",
        "animation_Q_learn = []\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  action = np.argmax(Q[state, :])\n",
        "  state, reward, done, info = env.step(action)\n",
        "  \n",
        "  animation_Q_learn.append({\"frame\": env.render(mode=\"ansi\"), \"state\":state,\n",
        "                    \"action\":action, \"reward\":reward})\n",
        "  epochs += 1\n",
        "\n",
        "print(\"The agent used {} timesteps for delivery.\".format(epochs))\n",
        "print(\"The agent got {} penalties.\".format(penalties))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The agent used 15 timesteps for delivery.\n",
            "The agent got 0 penalties.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qQL5IG2fRZ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "2a311c56-91c5-46f6-f277-4adb60ab5b1c"
      },
      "source": [
        "%%time\n",
        "# To animate, we define a function.\n",
        "def animating(frames, time_per_frame):\n",
        "  for i, frame in enumerate(frames):\n",
        "    clear_output(wait=True)\n",
        "    print(frame[\"frame\"].getvalue())\n",
        "    print(f\"Timesteps: {i}\")\n",
        "    print(f\"State: {frame['state']}\")\n",
        "    print(f\"Action: {frame['action']}\")\n",
        "    print(f\"Reward: {frame['reward']}\")\n",
        "    sleep(time_per_frame)\n",
        "    \n",
        "animating(animation_Q_learn, 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timesteps: 14\n",
            "State: 475\n",
            "Action: 5\n",
            "Reward: 20\n",
            "CPU times: user 47.2 ms, sys: 17.5 ms, total: 64.7 ms\n",
            "Wall time: 7.54 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73SeT8frfaFq"
      },
      "source": [
        "%%time\n",
        "# Let us implement expected Sarsa algorithm.\n",
        "# Define hyperparameters\n",
        "alpha = .4\n",
        "epsilon = .1\n",
        "gamma = .9\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "num_iterations = 100\n",
        "\n",
        "# Initialize\n",
        "Q = np.zeros([num_states, num_actions])\n",
        "\n",
        "# Loop for each episode\n",
        "for iter in range(num_iterations):\n",
        "  S = env.reset() # Initialize S\n",
        "  \n",
        "  # Loop for each step of episode\n",
        "  done = False\n",
        "  while not done:\n",
        "    # Epsilon-greedy XX\n",
        "    if np.random.rand() < epsilon:\n",
        "      A = env.action_space.sample() # take a random action\n",
        "    else:\n",
        "      A = np.argmax(Q[S, :])\n",
        "      \n",
        "    # Calculate Sarsa moving distance in expectation\n",
        "    tmp = 0\n",
        "    for a in range(Q.shape[-1]):\n",
        "      if a == A:\n",
        "        tmp += (1-epsilon)*Q[S_prime, a]\n",
        "      else:\n",
        "        tmp += epsilon*(1/(num_actions-1))*Q[S_prime, a]\n",
        "      \n",
        "    S_prime, R, done, info = env.step(int(A))\n",
        "    Q[S, A] += alpha * (R + gamma * tmp - Q[S, A])\n",
        "    S = S_prime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhxRQQsji_DG"
      },
      "source": [
        "%%time\n",
        "# Let us implement Double Q-learning algorithm.\n",
        "# Define hyperparameters\n",
        "alpha = .4\n",
        "epsilon = .1\n",
        "gamma = .9\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "num_iterations = 1000\n",
        "\n",
        "# Initialize\n",
        "Q1 = np.zeros([num_states, num_actions])\n",
        "Q2 = np.zeros([num_states, num_actions])\n",
        "\n",
        "# Loop for each episode\n",
        "for iter in range(num_iterations):\n",
        "  S = env.reset() # Initialize S\n",
        "  \n",
        "  # Loop for each step of episode\n",
        "  done = False\n",
        "  while not done:\n",
        "    # Epsilon-greedy XX\n",
        "    if np.random.rand() < epsilon:\n",
        "      A = env.action_space.sample() # take a random action\n",
        "    else:\n",
        "      tmp = Q1+Q2\n",
        "      A = np.argmax(tmp[S, :])\n",
        "    S_prime, R, done, info = env.step(int(A))\n",
        "    \n",
        "    # Flip a coin\n",
        "    if np.random.rand() < 0.5:\n",
        "      Q1[S, A] += alpha * (R + gamma * Q2[S_prime, np.argmax(Q1[S_prime, :])] - Q1[S, A])\n",
        "    else:\n",
        "      Q2[S, A] += alpha * (R + gamma * Q1[S_prime, np.argmax(Q2[S_prime, :])] - Q2[S, A])\n",
        "    S = S_prime"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}